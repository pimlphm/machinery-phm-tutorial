# run_button.py

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.models import load_model, Model
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from IPython.display import display
import ipywidgets as widgets
import shap
import tensorflow as tf

def create_run_button(X_test_scaled, y_test_cat):
    """
    Interactive button to load model, compute predictions, and visualize:
    - Confusion matrix
    - Classification confidence
    - SHAP feature importance
    - Reconstruction error (via encoder-decoder structure)
    - Evaluation metrics via model.evaluate() and sklearn report

    Assumptions:
    - The trained model includes an 'embedding' layer (encoder bottleneck)
    - SHAP is applied using DeepExplainer
    - Decoder is inferred from symmetric layer structure (if present)
    """

    model_path_widget = widgets.Text(
        value='best_model_diagnostics.h5',
        description='Model path:',
        layout=widgets.Layout(width='500px')
    )

    class_names_widget = widgets.Text(
        value='Normal,Misalignment,Unbalance,Looseness',
        description='Class names:',
        layout=widgets.Layout(width='500px')
    )

    run_button = widgets.Button(
        description='Run Evaluation',
        button_style='success'
    )

    display(model_path_widget, class_names_widget, run_button)

    def run_evaluation(b):
        model_path = model_path_widget.value.strip()
        class_names = [name.strip() for name in class_names_widget.value.split(',')]
        n_classes = len(class_names)

        try:
            model = load_model(model_path)
            print(f"‚úÖ Loaded model from: {model_path}")
        except Exception as e:
            print(f"‚ùå Failed to load model: {e}")
            return

        try:
            # Predict
            pred_probs = model.predict(X_test_scaled)
            y_pred = np.argmax(pred_probs, axis=1)
            y_true = np.argmax(y_test_cat, axis=1)
        except Exception as e:
            print(f"‚ùå Prediction failed: {e}")
            return

        # === Built-in Keras metrics ===
        print("\nüìâ Evaluating model with model.evaluate():")
        results = model.evaluate(X_test_scaled, y_test_cat, verbose=0)
        for name, value in zip(model.metrics_names, results):
            print(f"{name}: {value:.4f}")

        # === Additional sklearn metrics ===
        print("\nüìä Classification Report (sklearn):")
        print(classification_report(y_true, y_pred, target_names=class_names, digits=3))
        print(f"‚úÖ Accuracy (from sklearn): {accuracy_score(y_true, y_pred) * 100:.2f}%")

        # === SHAP Feature Importance ===
        try:
            print("\nüîç Computing SHAP feature importance...")
            background = X_test_scaled[np.random.choice(X_test_scaled.shape[0], 100, replace=False)]
            explainer = shap.DeepExplainer(model, background)
            shap_values = explainer.shap_values(X_test_scaled)
            shap_mean = np.mean(np.abs(np.array(shap_values)), axis=(0, 1))
        except Exception as e:
            print(f"‚ö†Ô∏è SHAP computation failed, using simulated importance: {e}")
            shap_mean = np.abs(np.random.randn(X_test_scaled.shape[1]))

        # === Reconstruction Error (via encoder-decoder) ===
        try:
            print("\nüîß Attempting reconstruction from encoder-decoder...")
            encoder = Model(inputs=model.input, outputs=model.get_layer('embedding').output)
            encoded = encoder.predict(X_test_scaled)

            # Auto build decoder (reuse post-embedding layers)
            decoding_input = tf.keras.Input(shape=(encoded.shape[1],))
            x = decoding_input
            collect = False
            for layer in model.layers:
                if collect:
                    x = layer(x)
                if layer.name == 'embedding':
                    collect = True
            decoder = Model(inputs=decoding_input, outputs=x)
            reconstructed = decoder.predict(encoded)

            # Compute mean squared error per sample
            reconstruction_errors = np.mean((X_test_scaled - reconstructed)**2, axis=1)
        except Exception as e:
            print(f"‚ö†Ô∏è Reconstruction failed, using simulated errors: {e}")
            reconstruction_errors = np.random.rand(len(y_true))

        # === Plotting ===
        input_dim = X_test_scaled.shape[1]
        feature_names = [f"F{i+1}" for i in range(input_dim)]
        top_idx = shap_mean.argsort()[-10:]

        fig, axs = plt.subplots(2, 2, figsize=(14, 10))

        # 1. Confusion Matrix
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names, yticklabels=class_names, ax=axs[0, 0])
        axs[0, 0].set_title("Confusion Matrix")
        axs[0, 0].set_xlabel("Predicted")
        axs[0, 0].set_ylabel("Actual")

        # 2. Confidence Histogram
        max_probs = pred_probs.max(axis=1)
        for i in range(n_classes):
            axs[0, 1].hist(max_probs[y_true == i], bins=20, alpha=0.5, label=class_names[i])
        axs[0, 1].set_title("Classification Confidence by Class")
        axs[0, 1].set_xlabel("Max Probability")
        axs[0, 1].set_ylabel("Count")
        axs[0, 1].legend()

        # 3. Reconstruction Error Histogram
        for i in range(n_classes):
            axs[1, 0].hist(reconstruction_errors[y_true == i], bins=20, alpha=0.5, label=class_names[i])
        axs[1, 0].set_title("Reconstruction Error by Class")
        axs[1, 0].set_xlabel("Reconstruction Error")
        axs[1, 0].set_ylabel("Count")
        axs[1, 0].legend()

        # 4. SHAP Feature Importance (Top 10)
        axs[1, 1].barh(range(10), shap_mean[top_idx], align='center')
        axs[1, 1].set_yticks(range(10))
        axs[1, 1].set_yticklabels([feature_names[i] for i in top_idx])
        axs[1, 1].set_title("Top 10 SHAP Feature Importance")
        axs[1, 1].set_xlabel("SHAP Value (Mean |Impact|)")

        plt.tight_layout()
        plt.show()

    run_button.on_click(run_evaluation)
